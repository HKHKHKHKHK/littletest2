{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests,threading,datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "\n",
    "\"\"\"\n",
    "1、抓取西刺代理网站的代理ip\n",
    "2、并根据指定的目标url,对抓取到ip的有效性进行验证\n",
    "3、最后存到指定的path\n",
    "\"\"\"\n",
    "\n",
    "# ------------------------------------------------------文档处理--------------------------------------------------------\n",
    "# 写入文档\n",
    "def write(path,text):\n",
    "    with open(path,'a', encoding='utf-8') as f:\n",
    "        f.writelines(text)\n",
    "        f.write('\\n')\n",
    "# 清空文档\n",
    "def truncatefile(path):\n",
    "    with open(path, 'w', encoding='utf-8') as f:\n",
    "        f.truncate()\n",
    "# 读取文档\n",
    "def read(path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        txt = []\n",
    "        for s in f.readlines():\n",
    "            txt.append(s.strip())\n",
    "    return txt\n",
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "# 计算时间差,格式: 时分秒\n",
    "def gettimediff(start,end):\n",
    "    seconds = (end - start).seconds\n",
    "    m, s = divmod(seconds, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    diff = (\"%02d:%02d:%02d\" % (h, m, s))\n",
    "    return diff\n",
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "# 返回一个随机的请求头 headers\n",
    "def getheaders():\n",
    "    user_agent_list = [ \\\n",
    "        \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/22.0.1207.1 Safari/537.1\" \\\n",
    "        \"Mozilla/5.0 (X11; CrOS i686 2268.111.0) AppleWebKit/536.11 (KHTML, like Gecko) Chrome/20.0.1132.57 Safari/536.11\", \\\n",
    "        \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.6 (KHTML, like Gecko) Chrome/20.0.1092.0 Safari/536.6\", \\\n",
    "        \"Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.6 (KHTML, like Gecko) Chrome/20.0.1090.0 Safari/536.6\", \\\n",
    "        \"Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/19.77.34.5 Safari/537.1\", \\\n",
    "        \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/536.5 (KHTML, like Gecko) Chrome/19.0.1084.9 Safari/536.5\", \\\n",
    "        \"Mozilla/5.0 (Windows NT 6.0) AppleWebKit/536.5 (KHTML, like Gecko) Chrome/19.0.1084.36 Safari/536.5\", \\\n",
    "        \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3\", \\\n",
    "        \"Mozilla/5.0 (Windows NT 5.1) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3\", \\\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_8_0) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3\", \\\n",
    "        \"Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1062.0 Safari/536.3\", \\\n",
    "        \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1062.0 Safari/536.3\", \\\n",
    "        \"Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3\", \\\n",
    "        \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3\", \\\n",
    "        \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3\", \\\n",
    "        \"Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.0 Safari/536.3\", \\\n",
    "        \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/535.24 (KHTML, like Gecko) Chrome/19.0.1055.1 Safari/535.24\", \\\n",
    "        \"Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/535.24 (KHTML, like Gecko) Chrome/19.0.1055.1 Safari/535.24\"\n",
    "    ]\n",
    "    UserAgent=random.choice(user_agent_list)\n",
    "    headers = {'User-Agent': UserAgent}\n",
    "    return headers\n",
    "# -----------------------------------------------------检查ip是否可用----------------------------------------------------\n",
    "def checkip(targeturl,ip):\n",
    "    headers =getheaders()  # 定制请求头\n",
    "    proxies = {\"http\": \"http://\"+ip, \"https\": \"http://\"+ip}  # 代理ip\n",
    "    try:\n",
    "        response=requests.get(url=targeturl,proxies=proxies,headers=headers,timeout=5).status_code\n",
    "        if response == 200 :\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "#-------------------------------------------------------获取代理方法----------------------------------------------------\n",
    "# 免费代理 XiciDaili\n",
    "def findip(type,pagenum,targeturl,path): # ip类型,页码,目标url,存放ip的路径\n",
    "    list={'1': 'http://www.xicidaili.com/nt/', # xicidaili国内普通代理\n",
    "          '2': 'http://www.xicidaili.com/nn/', # xicidaili国内高匿代理\n",
    "          '3': 'http://www.xicidaili.com/wn/', # xicidaili国内https代理\n",
    "          '4': 'http://www.xicidaili.com/wt/'} # xicidaili国外http代理\n",
    "    url=list[str(type)]+str(pagenum) # 配置url\n",
    "    headers = getheaders() # 定制请求头\n",
    "    html=requests.get(url=url,headers=headers,timeout = 5).text\n",
    "    soup=BeautifulSoup(html,'lxml')\n",
    "    all=soup.find_all('tr',class_='odd')\n",
    "    for i in all:\n",
    "        t=i.find_all('td')\n",
    "        ip=t[1].text+':'+t[2].text\n",
    "        is_avail = checkip(targeturl,ip)\n",
    "        if is_avail == True:\n",
    "            write(path=path,text=ip)\n",
    "            print(ip)\n",
    "\n",
    "#-----------------------------------------------------多线程抓取ip入口---------------------------------------------------\n",
    "def getip(targeturl,path):\n",
    "     truncatefile(path) # 爬取前清空文档\n",
    "     start = datetime.datetime.now() # 开始时间\n",
    "     threads=[]\n",
    "     for type in range(4):   # 四种类型ip,每种类型取前三页,共12条线程\n",
    "         for pagenum in range(3):\n",
    "             t=threading.Thread(target=findip,args=(type+1,pagenum+1,targeturl,path))\n",
    "             threads.append(t)\n",
    "     print('开始爬取代理ip')\n",
    "     for s in threads: # 开启多线程爬取\n",
    "         s.start()\n",
    "     for e in threads: # 等待所有线程结束\n",
    "         e.join()\n",
    "     print('爬取完成')\n",
    "     end = datetime.datetime.now() # 结束时间\n",
    "     diff = gettimediff(start, end)  # 计算耗时\n",
    "     ips = read(path)  # 读取爬到的ip数量\n",
    "     print('一共爬取代理ip: %s 个,共耗时: %s \\n' % (len(ips), diff))\n",
    "\n",
    "#-------------------------------------------------------启动-----------------------------------------------------------\n",
    "if __name__ == '__main__':\n",
    "    path = 'ip.txt' # 存放爬取ip的文档path\n",
    "    targeturl = 'http://liepin.com/' # 验证ip有效性的指定url\n",
    "    getip(targeturl,path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
